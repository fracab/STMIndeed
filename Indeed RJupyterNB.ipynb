{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "STM is a text mining technique, initially conceived for the analysis of political texts, which has been extensively adopted in social sciences (<a href=\"https://www.structuraltopicmodel.com/\">here</a> you can find a list of the main publications that have adopted STM). As other topic models, like Latent Dirichlet Allocation it basically allows to identify abstract \"topics\" that occur in a collection of documents, but compared to other models, it allows the analysis of relationships with document metadata, in form of co-variates, either in terms of the degree of association of a document to a topic, either of the association of a word to a topic. As an example, it is possible to take a bunch of posts published on different political blogs in the months before an election, and see which topics were prevalent in the posts of blogs of a certain political leaning (in this case, political leaning of the blog is used as co-variate for topical prevalence), or to see how words associated to the treatment of a specific topic change depending to the political affiliation (in this case, political leaning of the blog is used as co-variate for topical content) (you can refer to the  <a href=\"https://cran.r-project.org/web/packages/stm/vignettes/stmVignette.pdf\">R package vignette</a> for more details).\n",
    "\n",
    "As I will stress all over, as the sample is totally arbitrary, any result will not have any statistical validity whatsoever. This is simply meant to be an attempt to explore a technique (and a R package) which can have several potential applications, both in terms of analytical purposes, both in terms of information visualisation (allowing for example to get over the use and abused word cloud). \n",
    "\n",
    "As many topics in data analysis, it is something easier to do rather than to explain, and something that can be really understood only when you get your hands dirty with some data, which is what prompted me to try this. In this and the following entries, I will try my hand and post the results of some attempts I am making with STM, more specifically testing the model on some job offers extracted from Indeed UK. As this is a work in progress, I will post the results of my work as I get through them, so I am not really sure where this will lead, but I hope to have some fun along the way. \n",
    "\n",
    "\n",
    "## Part I - Web Scraping\n",
    "\n",
    "In this first post I will not really get my hands on STM yet, but I will illustrate how I obtained the textual data I will use in the rest of the work. As mentioned above, I decided to focus on job offers: there is no specific reason for this, other than that I considered this a good example of texts that offer some metadata to incorporate in the analysis (type of job, salary, location), and whose topicality identification could represent a good test for the model. The choice fell on indeed.co.uk also for no specific reason, and I stuck to it after I noticed that scraping it was relatively easy (although the quality of metadata, as we will see later on, is not the best we could have hoped for). \n",
    "Obviously, if I had access to indeed.co.uk API this whole process would have been probably quicker, but since I don’t, I scraping the site was the only feasible option. \n",
    "The first step was to create three accessory functions to obtain from each offer page the information needed. This was relatively easily done thanks to htmlParse from XML package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load necessary libraries\n",
    "suppressWarnings(library(rvest))\n",
    "suppressWarnings(library(\"xml2\"))\n",
    "suppressWarnings(library(\"XML\"))\n",
    "suppressWarnings(library(\"stringr\"))\n",
    "suppressWarnings(library(dplyr))\n",
    "suppressWarnings(library(naniar))\n",
    "\n",
    "## Scrape the info from pages:\n",
    "#metadata \n",
    "getmetadataindeed<-function(url){\n",
    "  meta<-read_html(url)%>%\n",
    "    htmlParse( asText=TRUE)%>%xpathSApply( \"//*[contains(@class,'jobsearch-JobMetadataHeader-iconLabel')]\", xmlValue)\n",
    "  if (is.list(meta)) {meta<-NA\n",
    "  } #as not all the job descriptions contain metadata, this was introduced to avoid ending up with empty lists in the dataframe\n",
    "meta \n",
    "  }\n",
    "\n",
    "#job description\n",
    "getjobdescindeed<-function(url){\n",
    "  read_html(url)%>%\n",
    "    htmlParse( asText=TRUE)%>%xpathSApply( \"//*[contains(@id,'jobDescriptionText')]\", xmlValue)%>%paste(collapse=', ')%>%str_replace_all(\"\\n\", \" \")\n",
    "  \n",
    "}\n",
    "\n",
    "#job title \n",
    "getjobtitle<-function(url){\n",
    "  tit<-read_html(url)%>%\n",
    "    htmlParse( asText=TRUE)%>%xpathSApply( \"//*[contains(@class, 'icl-u-xs-mb--xs icl-u-xs-mt--none jobsearch-JobInfoHeader-title')]\", xmlValue)\n",
    "  if (is.list(tit)) {\n",
    "    tit<-NA\n",
    "  }\n",
    "  tit\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As these functions need to be fed with the specific URLs of the job offer webpages, and do this by hand for hundreds of offers is not really an option, I created a second function to directly scrape the URLs from the results page of a search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "getlinks<-function(url){\n",
    "  linksb<-read_html(url)%>%htmlParse(asText = TRUE)%>%xmlRoot()%>%xpathSApply(\"//*[contains(@class,'title')]\", xmlGetAttr, 'href')\n",
    "  linksb[sapply(linksb, is.null)] <- NULL\n",
    "  linksb<-as.character(linksb)\n",
    "  linksb<-paste(\"https://www.indeed.co.uk\", linksb,sep=\"\")\n",
    "    } "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, as some sponsored links are always present in the results page, the total number of job offer URLs scraped is slightly superior to the expected number, which for our purpose doesn’t really create particular issues. The final step was to put everything together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapeindeed<-function(urlres) {\n",
    "  linksbb<-getlinks(urlres)\n",
    "  jobtitles<-lapply(FUN=getjobtitle, linksbb)%>%plyr::ldply(rbind)%>%mutate_if(is.factor,as.character)\n",
    "  jobsdesc<-lapply (FUN=getjobdescindeed, linksbb)%>%plyr::ldply(rbind)%>%mutate_if(is.factor,as.character)\n",
    "  jobmeta<-lapply (FUN=getmetadataindeed, linksbb)%>%plyr::ldply(rbind)%>%mutate_if(is.factor,as.character)\n",
    "  tobemoved <- grepl(\"£\", jobmeta[,2])#as salary can fall in the second column if location is missing, single out all the salary entries…\n",
    "  jobmeta[tobemoved,3 ]<-jobmeta[tobemoved,2 ] #...to move them to the third column\n",
    "  jobmeta[tobemoved,2 ] <-NA #leaving a NA on their place in the second column \n",
    "  final<-cbind(jobtitles,jobsdesc,jobmeta)%>%`colnames<-`(c(\"Title\", \"Description\", \"Location\",\"Type\",\"Salary\"))\n",
    "  final\n",
    "}\n",
    "\n",
    "###############\n",
    "#test \n",
    "##\n",
    "results1<-scrapeindeed(\"https://www.indeed.co.uk/jobs?as_and=&as_phr=&as_any=&as_not=&as_ttl=&as_cmp=&jt=all&st=&as_src=&salary=&radius=25&l=ne18&fromage=3&limit=50&sort=&psf=advsrch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print.table(results1[1,], justify=\"left\", width=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function, fed with the URL of the results page, can extract all the data needed and store them in a dataframe of five columns. The last step was to fed the function with all the results pages relevant, which I did (in this case manually), for all the job offers published in the last three days before Saturday 18th May within 25 miles of the postcode NE18 (Newcastle Upon Tyne). The results were then merged together, the data are available <a href=\"https://www.dropbox.com/s/fa3hhcfi5qkqfp1/totaljobs.txt?dl=0\">here</a>  in .txt format.\n",
    "\n",
    "As you can see, there is still much to do in terms of data cleaning before starting to work, which is what we will see in the next section. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II – Data cleaning\n",
    "\n",
    "We have now a still rather messy dataframe requiring some data cleaning before being ready to be used. Although the description part can be considered OK to be processed within STM or some other package specifically meant to work with textual data (like <a href=\"https://quanteda.io/\">quanteda</a>), the metadata requires further massaging. Here I focus mostly on location (as I am interested in trying to perform some kind of spatial data analysis), and the salary column, as it contains quantitative information that is basically impossible to analyse in its present form. \n",
    "\n",
    "### Location\n",
    "The location metadata is rather vague and not uniform (for most offers, the municipality is provided, but for other entries the region is mentioned instead, only a very few report the postcode), so what we can in fact do with it is rather limited. However, as this is an experiment to have fun and explore, we can try to make the most out of it nevertheless, testing what could then be achieved with better data. \n",
    "What I try here is basically to extrapolate the coordinates corresponding to the location of each offer, so that it can easily be geo-referred. This can be achieved relatively easily with <a href=\"https://www.r-bloggers.com/osm-nominatim-with-r-getting-locations-geo-coordinates-by-its-address/\">nominatim_osm</a>, a little sweet function by Dmitry Kisler which exploit the OpenStreetMap API (there is an alternative based on google geocoding api implemented in ggmap, but it has a limit of 2500 requests per day).  The function works best with full addresses, but the municipality name is enough to get the coordinates of this (as far as it is present in OSM database). On the other hand, it doesn’t really like postcodes, so we preliminary remove them from the location column, extracting all the strings containing a digit (for convenience I have loaded the dataset prepared before, with the offers published in the last three days before Saturday 18th May within 25 miles of the postcode NE18):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'Durham'"
      ],
      "text/latex": [
       "'Durham'"
      ],
      "text/markdown": [
       "'Durham'"
      ],
      "text/plain": [
       "[1] \"Durham\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "suppressWarnings(library(jsonlite))\n",
    "suppressWarnings(library(plyr))\n",
    "suppressWarnings(library(sjmisc))\n",
    "suppressWarnings(library(readr))\n",
    "suppressWarnings(library(tidyr))\n",
    "suppressWarnings(library(kulife))\n",
    "suppressWarnings(library(stringr))\n",
    "suppressWarnings(library(glue))\n",
    "suppressWarnings(library(dplyr))\n",
    "suppressWarnings(library(purrr))\n",
    "\n",
    "\n",
    "totaljobs<-read.table(\"totaljobs.txt\")# we recover the data previously scraped\n",
    "totaljobs<-totaljobs[!duplicated(totaljobs),] # get rid of duplicates \n",
    "totaljobs$Location<-(str_extract(totaljobs$Location, \"(\\\\b[^\\\\d]+\\\\b)\")) # extract only the words without digits\n",
    "totaljobs$Location[24]#verify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can directly fed the dataframe to the nominatim_osm function (the version I use here has only a small modification to ignore NA values, note that it might be a bit time consuming depending on speed of connection and size of the dataset):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "nominatim_osmMod <- function(address = NULL)\n",
    "{\n",
    "  if(suppressWarnings(is.na(address)))\n",
    "    return(data.frame())\n",
    "  tryCatch(\n",
    "    d <- jsonlite::fromJSON( \n",
    "      gsub('\\\\@addr\\\\@', gsub('\\\\s+', '\\\\%20', address), \n",
    "           'http://nominatim.openstreetmap.org/search/@addr@?format=json&addressdetails=0&limit=1')\n",
    "    ), error = function(c) return(data.frame())\n",
    "  )\n",
    "  if(length(d) == 0) return(data.frame())\n",
    "  return(data.frame(lon = as.numeric(d$lon), lat = as.numeric(d$lat)))\n",
    "}#slightly modified to deal with NA instead of NULL\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "JobsCoord<- lapply(totaljobs$Location, nominatim_osmMod)\n",
    "filter<-lapply(JobsCoord, is_empty)\n",
    "JobsCoord[unlist(filter)]<-0\n",
    "JobsCoorddf <- ldply(JobsCoord, data.frame)\n",
    "\n",
    "JobsCoorddf[3]<-1:length(JobsCoorddf[,1])#ordinal index for coord\n",
    "totaljobs[,6]<-(1:length(totaljobs[,1]))#ordinal index for totaljobs\n",
    "\n",
    "names(totaljobs)[6]<-\"OrdIndex\"\n",
    "names(JobsCoorddf)[3]<-\"OrdIndex\"\n",
    "\n",
    "totaljobsCoord <- merge(totaljobs,JobsCoorddf, by=\"OrdIndex\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "write.table(totaljobsCoord, \"totaljobsCoord.txt\",sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting dataset has the coordinates for the entity in the location column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>Location</th><th scope=col>Type</th><th scope=col>Salary</th><th scope=col>lon</th><th scope=col>lat</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>23</th><td>Newcastle upon Tyne     </td><td>NA                      </td><td>£21,414 - £22,658 a year</td><td>-1.613157               </td><td>54.97385                </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllll}\n",
       "  & Location & Type & Salary & lon & lat\\\\\n",
       "\\hline\n",
       "\t23 & Newcastle upon Tyne      & NA                       & £21,414 - £22,658 a year & -1.613157                & 54.97385                \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | Location | Type | Salary | lon | lat | \n",
       "|---|\n",
       "| 23 | Newcastle upon Tyne      | NA                       | £21,414 - £22,658 a year | -1.613157                | 54.97385                 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "   Location            Type Salary                   lon       lat     \n",
       "23 Newcastle upon Tyne NA   £21,414 - £22,658 a year -1.613157 54.97385"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "totaljobsCoord[23,4:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a final issue to be noted, as in case of homonyms the osm API returns the most relevant result, which might not be the one relevant in our case. In the data we are using, for example, there is a “Washington” which happens to be a town in Tyne and Wear, which is unlikely to be considered the most relevant result by osm. As the entries in question are relatively few, I opted for manual intervention for most cases, an alternative workaround could be to add “, UK” at the end of each location cell, although it is a solution not devoid of side effects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salary\n",
    "\n",
    "In its present form, the salary column is basically useless for any practical purposes, as we have rates by different units (from hours to year), in some cases a single value is given, in others a range.  \n",
    "The first step I took to sort this out is to extract the numeric values from the column. This can be done with grepexpr and string, which are conveniently combined in the <a href=\"http://stla.github.io/stlapblog/posts/Numextract.html#\">Numextract function</a> by Ramnath Vaidyanathan (here I used the dataframe with the coordinates columns, but of course this procedure will work on the original dataframe as well):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "totaljobsCoord$Salary<-as.character(totaljobsCoord$Salary)\n",
    "totaljobsCoord$Salary<-gsub(\",\", \"\", totaljobsCoord$Salary)#remove commas from digits in thousands\n",
    "\n",
    "Numextract <- function(string){\n",
    "  unlist(regmatches(string,gregexpr(\"[[:digit:]]+\\\\.*[[:digit:]]*\",string)))\n",
    "}#curtesy of http://stla.github.io/stlapblog/posts/Numextract.html\n",
    "\n",
    "\n",
    "totaljobsCoord$rate<-lapply(totaljobsCoord$Salary, Numextract)%>%plyr::ldply(rbind)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are stored in a dataframe within a dataframe, which we'd better store in two distinct columns for ease of use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "totaljobsCoord$minrate<-as.numeric(as.character(unlist(totaljobsCoord$rate[1])))#min rate\n",
    "totaljobsCoord$maxrate<-as.numeric(as.character(unlist(totaljobsCoord$rate[2])))#max rate\n",
    "\n",
    "totaljobsCoord$maxrate[!complete.cases(totaljobsCoord$maxrate)]<-totaljobsCoord$minrate[!complete.cases(totaljobsCoord$maxrate)]#in case there is no range but only a single entry, the same value is repeted in both columns\n",
    "\n",
    "totaljobsCoordRates <- select(totaljobsCoord, -rate)#get rid of the original\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to create a column extracting the factor by which the rate is computed, which can be relatively easy done with str_extract. We can preliminary check which factors to look for:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [1] \"£ a year\"        \"£. - £. an hour\" \"£ an hour\"       NA               \n",
      " [5] \"£. an hour\"      \"£ - £ a year\"    \"£ - £ an hour\"   \"£ a week\"       \n",
      " [9] \"£ - £ a month\"   \"£ a day\"         \"£ - £ a day\"     \"£. - £. a day\"  \n"
     ]
    }
   ],
   "source": [
    "print(unique(gsub(\"\\\\b\\\\d+\\\\b\",\"\",totaljobsCoord$Salary))#find out the possible factors\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "totaljobsCoordRates$rateby<-(str_extract(totaljobsCoord$Salary, \"(\\\\b year|day|month|week|hour\\\\b)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes the data more easily comparable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>Salary</th><th scope=col>minrate</th><th scope=col>maxrate</th><th scope=col>rateby</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>£28283 a year        </td><td>28283.0              </td><td>28283.0              </td><td> year                </td></tr>\n",
       "\t<tr><td>£8.50 - £8.70 an hour</td><td>    8.5              </td><td>    8.7              </td><td>hour                 </td></tr>\n",
       "\t<tr><td>£18000 a year        </td><td>18000.0              </td><td>18000.0              </td><td> year                </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llll}\n",
       " Salary & minrate & maxrate & rateby\\\\\n",
       "\\hline\n",
       "\t £28283 a year         & 28283.0               & 28283.0               &  year                \\\\\n",
       "\t £8.50 - £8.70 an hour &     8.5               &     8.7               & hour                 \\\\\n",
       "\t £18000 a year         & 18000.0               & 18000.0               &  year                \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "Salary | minrate | maxrate | rateby | \n",
       "|---|---|---|\n",
       "| £28283 a year         | 28283.0               | 28283.0               |  year                 | \n",
       "| £8.50 - £8.70 an hour |     8.5               |     8.7               | hour                  | \n",
       "| £18000 a year         | 18000.0               | 18000.0               |  year                 | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  Salary                minrate maxrate rateby\n",
       "1 £28283 a year         28283.0 28283.0  year \n",
       "2 £8.50 - £8.70 an hour     8.5     8.7 hour  \n",
       "3 £18000 a year         18000.0 18000.0  year "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "totaljobsCoordRates[c(1:3),c(6,9,10,11)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final step, we store the resulting dataframe in a .txt for future retrieval. Depending on the type of analysis we want to perform, there is still further cleaning and massaging to be done, but for now this can be considered a workable starting point. In the next sessions I will start to use STM to do some actual analysis on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write.table(totaljobsCoordRates,\"totaljobsCoordRates.txt\",sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "In the repo you can find the procedures seen above as functions:\n",
    "- <a href=\"https://github.com/fracab/STMIndeed/blob/master/CoordIndeedG.R\">CoordIndeedG.R</a>: extrapolate coordinates and store them in two new columns;\n",
    "- <a href=\"https://github.com/fracab/STMIndeed/blob/master/CleanSalaryIndeedG.R\">CleanSalaryIndeedG.R</a>: extrapolate min and max salary and factor by which the rate is computed;\n",
    "- <a href=\"https://github.com/fracab/STMIndeed/blob/master/CleanIndeedG.R\">CleanIndeedG.R</a>: incorporate both functions\n",
    "- <a href=\"https://github.com/fracab/STMIndeed/blob/master/ScrapingIndeedCodeG.R\">ScrapingIndeedCodeG.R</a>: the function presented in the first section to scrape data from Indeed. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
